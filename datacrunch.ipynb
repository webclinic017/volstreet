{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import volstreet.datamodule as dm\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from datetime import time, datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import volstreet as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using DataClient class\n",
    "client = dm.DataClient(api_key=__import__('os').environ['EOD_API_KEY'])\n",
    "kite_user = __import__('os').environ['KITE_USER']\n",
    "kite_pass = __import__('os').environ['KITE_PASS']\n",
    "kite_api_key = __import__('os').environ['KITE_API_KEY']\n",
    "kite_api_secret = __import__('os').environ['KITE_API_SECRET']\n",
    "kite_auth_key = __import__('os').environ['KITE_AUTH_KEY']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using get_data and analyser functions\n",
    "nifty_data = client.get_data(symbol='NIFTY')\n",
    "bnf_data = client.get_data(symbol='BANKNIFTY')\n",
    "finnifty_data = client.get_data(symbol='FINNIFTY')\n",
    "nifty_daily_data = dm.analyser(nifty_data, frequency='D')\n",
    "bnf_daily_data = dm.analyser(bnf_data, frequency='D')\n",
    "nifty_weekly_data = dm.analyser(nifty_data, frequency='W-THU')\n",
    "bnf_weekly_data = dm.analyser(bnf_data, frequency='W-THU')\n",
    "nifty_monthly_data = dm.analyser(nifty_data, frequency='M-THU')\n",
    "bnf_monthly_data = dm.analyser(bnf_data, frequency='M-THU')\n",
    "\n",
    "vix = client.get_data(\"VIX\", return_columns=[\"open\", \"close\"])\n",
    "vix = vix.resample(\"B\").ffill()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using ratio_analysis function\n",
    "rolling_periods = 5\n",
    "ratio_data = dm.ratio_analysis(bnf_weekly_data, nifty_weekly_data, add_rolling=rolling_periods)\n",
    "px.line(ratio_data, x=ratio_data.index, y=['BANKNIFTY Change', 'NIFTY Change', f'Rolling {rolling_periods} Ratio'], color_discrete_map={'BANKNIFTY Change': 'red', 'NIFTY Change': 'blue', f'Rolling {rolling_periods} Ratio': 'green'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ratio_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using gambler function for NIFTY and BANKNIFTY\n",
    "for index in ['NIFTY', 'BANKNIFTY']:\n",
    "    print(f'{index}\\n')\n",
    "    data = client.get_data(symbol=index)\n",
    "    for frequency in ['D', 'D-THU', 'W-THU', 'M-THU']:\n",
    "        print(f'{frequency}\\n')\n",
    "        dm.gambler(data, frequency, 'abs_change')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using gambler function for FINNIFTY\n",
    "for index in ['FINNIFTY']:\n",
    "    print(f'{index}\\n')\n",
    "    data = client.get_data(symbol=index)\n",
    "    for frequency in ['D', 'D-TUE', 'W-TUE', 'M-TUE']:\n",
    "        print(f'{frequency}\\n')\n",
    "        dm.gambler(data, frequency, 'abs_change')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rolling average of absolute change to support gambler function\n",
    "analysed_df = dm.analyser(finnifty_data, frequency='W-tue')\n",
    "rolling_periods = 5\n",
    "analysed_df['rolling'] = analysed_df['abs_change'].rolling(rolling_periods, min_periods=1).mean()\n",
    "#fig = px.line(analysed_df, x=analysed_df.index, y='rolling')\n",
    "#fig.add_hline(y=analysed_df['abs_change'].mean(), line_dash=\"dot\", annotation_text=\"Mean\", #annotation_position=\"top right\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analysed_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One min data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kite_obj = dm.get_greenlit_kite(kite_api_key, kite_api_secret, kite_user, kite_pass, kite_auth_key)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Updating one min data for NIFTY 50, NIFTY BANK and NIFTY FIN SERVICE\n",
    "dm.get_1m_data(kite_obj, 'NIFTY 50', path='data\\\\')\n",
    "dm.get_1m_data(kite_obj, 'NIFTY BANK', path='data\\\\')\n",
    "dm.get_1m_data(kite_obj, 'NIFTY FIN SERVICE', path='data\\\\')\n",
    "dm.get_1m_data(kite_obj, 'NIFTY MID SELECT', path='data\\\\')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dm.get_constituent_1m_data(kite_obj, 'NIFTY', path='data\\\\')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nifty_onemin = pd.read_csv('data/NIFTY 50_onemin_prices.csv', index_col=0, parse_dates=True)\n",
    "bnf_onemin = pd.read_csv('data/NIFTY BANK_onemin_prices.csv', index_col=0, parse_dates=True)\n",
    "fin_onemin = pd.read_csv('data/NIFTY FIN SERVICE_onemin_prices.csv', index_col=0, parse_dates=True)\n",
    "midcp_onemin = pd.read_csv('data/NIFTY MID SELECT_onemin_prices.csv', index_col=0, parse_dates=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Intraday Trend"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_nifty = dm.backtest_intraday_trend(nifty_onemin, vix, beta = 0.8, rolling_days=90, max_entries=10)\n",
    "trend_bnf = dm.backtest_intraday_trend(bnf_onemin, vix, beta = 0.8, rolling_days=90, max_entries=10)\n",
    "trend_finnifty = dm.backtest_intraday_trend(fin_onemin, vix, beta = 0.8, rolling_days=90, max_entries=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_indices_returns = trend_bnf.merge(trend_finnifty, left_index=True, right_index=True, suffixes=('_bnf', '_finnifty')).merge(trend_nifty, left_index=True, right_index=True, suffixes=('', '_nifty'))\n",
    "# Selecting only the columns of interest (minute vol, open to close trend, returns)\n",
    "all_indices_returns.filter(regex='returns').sum(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_bnf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Year wise summary of returns\n",
    "df_to_sum = trend_nifty\n",
    "df_to_sum.groupby(df_to_sum.index.year).total_returns.sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the distribution of returns for various entries\n",
    "df_to_plot = trend_finnifty\n",
    "returns_1 = df_to_plot.trade_data.apply(lambda x: x.get('entry_1', {}).get('returns', np.nan)).dropna()\n",
    "returns_2 = df_to_plot.trade_data.apply(lambda x: x.get('entry_2', {}).get('returns', np.nan)).dropna()\n",
    "returns_3 = df_to_plot.trade_data.apply(lambda x: x.get('entry_3', {}).get('returns', np.nan)).dropna()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=returns_1, name='Entry 1', nbinsx=10))\n",
    "fig.add_trace(go.Histogram(x=returns_2, name='Entry 2', nbinsx=10))\n",
    "fig.add_trace(go.Histogram(x=returns_3, name='Entry 3', nbinsx=10))\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the distribution of the ratio for stop loss and no stop loss days for all indices\n",
    "all_indices_with_drivers = pd.concat([trend_nifty, trend_bnf, trend_finnifty])\n",
    "all_indices_with_drivers_stop_loss =  all_indices_with_drivers[(all_indices_with_drivers.total_returns <= 0)]\n",
    "all_indices_with_drivers_no_stop_loss =  all_indices_with_drivers[(all_indices_with_drivers.total_returns > 0)]\n",
    "fig = px.histogram(all_indices_with_drivers_stop_loss, x='ratio')\n",
    "fig.add_trace(go.Histogram(x=all_indices_with_drivers_no_stop_loss.ratio, name='No Stop Loss'))\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the rolling ratio of three indices\n",
    "df_to_plot = all_indices_returns\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Line(x=df_to_plot.index, y=df_to_plot['rolling_ratio_bnf'], name='BNF'))\n",
    "fig.add_trace(go.Line(x=df_to_plot.index, y=df_to_plot['rolling_ratio_finnifty'], name='Fin Nifty'))\n",
    "fig.add_trace(go.Line(x=df_to_plot.index, y=df_to_plot['rolling_ratio'], name='Nifty'))\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the minute vol and open to close trend on different y axis\n",
    "df_to_plot = trend_finnifty\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(go.Line(x=df_to_plot.index, y=df_to_plot['rolling_ratio'], name='Ratio'), secondary_y=True)\n",
    "fig.add_trace(go.Line(x=df_to_plot.index, y=df_to_plot['strat_nav'], name='Nav'), secondary_y=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trying different beta values\n",
    "for beta in range(80, 105, 5):\n",
    "    beta = beta/100\n",
    "    trend_nifty = dm.backtest_intraday_trend(nifty_onemin, vix, beta = beta)\n",
    "    print(f'Beta: {beta}')\n",
    "    print(f'NIFTY: {trend_nifty[\"total_returns\"].sum()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If random strategy is truly profitable then which are the best parameters\n",
    "results = {}\n",
    "for open_candle in range(0, 10):\n",
    "    results[open_candle] = []\n",
    "    for _ in range(10):\n",
    "        bnf_random = dm.backtest_intraday_trend(bnf_onemin, vix, open_nth = open_candle, randomize=True)\n",
    "        total_rets = bnf_random.total_returns.sum()\n",
    "        results[open_candle].append(total_rets)\n",
    "    print(f'Open candle: {open_candle}')\n",
    "    print(f'Mean: {np.mean(results[open_candle])}')\n",
    "    print(f'Std: {np.std(results[open_candle])}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Intraday one minute volatility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_vol_to_one_min_data(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    def return_vol_and_rolling_vol(group):\n",
    "        group['vol'] = group['close'].pct_change().abs()*100\n",
    "        # taking cumulative mean of vol\n",
    "        group['rolling_vol'] = group['close'].pct_change().abs().expanding().mean()*100\n",
    "        return group\n",
    "    \n",
    "    dataframe= dataframe.groupby(dataframe.index.date).apply(return_vol_and_rolling_vol)\n",
    "    dataframe = dataframe.reset_index(level=0, drop=True)\n",
    "    return dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nifty_onemin = add_vol_to_one_min_data(nifty_onemin)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nifty_onemin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trial_df = nifty_onemin.loc['2015':]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def identify_dull_spurts(data, threshold_pct=0.6, threshold_window_size=90, desired_move=0.0027):\n",
    "    \n",
    "    print(f\"Identifying dull periods and spurts for {data['date'].iloc[0].date()}\")\n",
    "    \n",
    "    # Initialize columns for labeling dull periods and spurts\n",
    "    data['dull_period'] = 0\n",
    "    data['successful_exit'] = 0\n",
    "    data['rolling_vol_window_mean'] = 0\n",
    "    data['rolling_vol_prior'] = 0\n",
    "    data['dull_threshold'] = 0\n",
    "    # data['subsequent_vol'] = 0\n",
    "    data['spot_at_entry'] = 0\n",
    "    data['upper_bound'] = 0\n",
    "    data['lower_bound'] = 0\n",
    "    data['spot_at_exit'] = 0\n",
    "\n",
    "    # Iterate through the data to identify dull periods and subsequent spurts\n",
    "    i = 0\n",
    "    while i < len(data) - 1:\n",
    "        \n",
    "        # Skip the scanning process if before 10 AM\n",
    "        if data.iloc[i, data.columns.get_loc('date')].time() < pd.Timestamp(\"12:20\").time():\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        rolling_vol_window_mean = data.iloc[max(0, i-threshold_window_size):i, data.columns.get_loc('vol')].mean()\n",
    "        rolling_vol_prior = data.iloc[max(0, i-threshold_window_size), data.columns.get_loc('rolling_vol')]\n",
    "        dull_threshold = rolling_vol_prior * threshold_pct\n",
    "        data.iloc[i, data.columns.get_loc('rolling_vol_window_mean')] = rolling_vol_window_mean\n",
    "        data.iloc[i, data.columns.get_loc('rolling_vol_prior')] = rolling_vol_prior\n",
    "        data.iloc[i, data.columns.get_loc('dull_threshold')] = dull_threshold\n",
    "        if rolling_vol_window_mean < dull_threshold:\n",
    "            # Mark the dull period\n",
    "            data.iloc[i, data.columns.get_loc('dull_period')] = 1\n",
    "            spot_at_entry = data.iloc[i, data.columns.get_loc('close')]\n",
    "            data.iloc[i, data.columns.get_loc('spot_at_entry')] = spot_at_entry\n",
    "            upper_bound = spot_at_entry * (1 + desired_move)\n",
    "            lower_bound = spot_at_entry * (1 - desired_move)\n",
    "            data.iloc[i, [data.columns.get_loc('upper_bound'), data.columns.get_loc('lower_bound')]] = (upper_bound, lower_bound)\n",
    "            \n",
    "            future_prices = data.iloc[i+1:-1]\n",
    "            # Check if the price has moved beyond the upper or lower bound in future prices\n",
    "            if future_prices['high'].max() > upper_bound or future_prices['low'].min() < lower_bound:\n",
    "                # Check whether the price has moved beyond the upper or lower bound and if both then \n",
    "                # which one first\n",
    "                if future_prices['high'].max() > upper_bound and future_prices['low'].min() < lower_bound:\n",
    "                    \n",
    "                    high_crossing_index = future_prices[future_prices['high'] > upper_bound].first_valid_index()\n",
    "                    low_crossing_index = future_prices[future_prices['low'] < lower_bound].first_valid_index()\n",
    "                    first_crossing_index = min(high_crossing_index, low_crossing_index)\n",
    "                    exit_price = future_prices.loc[first_crossing_index, 'high'] if first_crossing_index == high_crossing_index else future_prices.loc[first_crossing_index, 'low']\n",
    "                    data.iloc[first_crossing_index, data.columns.get_loc('spot_at_exit')] = exit_price\n",
    "                    data.iloc[first_crossing_index, data.columns.get_loc('successful_exit')] = 1 if first_crossing_index == high_crossing_index else -1\n",
    "\n",
    "                elif future_prices['high'].max() > upper_bound:\n",
    "                    crossing_index = future_prices[future_prices['high'] > upper_bound].first_valid_index()\n",
    "                    exit_price = future_prices.loc[crossing_index, 'high']\n",
    "                    data.iloc[crossing_index, data.columns.get_loc('spot_at_exit')] = exit_price\n",
    "                    data.iloc[crossing_index, data.columns.get_loc('successful_exit')] = 1\n",
    "                    \n",
    "                elif future_prices['low'].min() < lower_bound:\n",
    "                    crossing_index = future_prices[future_prices['low'] < lower_bound].first_valid_index()\n",
    "                    exit_price = future_prices.loc[crossing_index, 'low']\n",
    "                    data.iloc[crossing_index, data.columns.get_loc('spot_at_exit')] = exit_price\n",
    "                    data.iloc[crossing_index, data.columns.get_loc('successful_exit')] = 1\n",
    "                \n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            else:\n",
    "                data.iloc[-1, data.columns.get_loc('spot_at_exit')] = data.iloc[-1, data.columns.get_loc('close')]\n",
    "                data.iloc[-1, data.columns.get_loc('successful_exit')] = -1\n",
    "            \n",
    "            break\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "daily_data = [day for _, day in trial_df.groupby(trial_df.index.date)]\n",
    "# Applying the updated function to each day's data and concatenating the results\n",
    "labeled_daily_data = pd.concat([identify_dull_spurts(day.reset_index(), threshold_pct=0.4, threshold_window_size=120) for day in daily_data])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labeled_daily_data_grouped = labeled_daily_data.groupby(labeled_daily_data['date'].dt.date).agg({'dull_period': 'sum', 'successful_exit': 'sum'})\n",
    "labeled_daily_data_grouped.sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_labeled_data = labeled_daily_data_grouped[labeled_daily_data_grouped['dull_period'] > 0]\n",
    "iter_obj = iter(filtered_labeled_data.index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labeled_daily_data.set_index('date')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "date_to_plot = next(iter_obj)\n",
    "target_day_data = labeled_daily_data[labeled_daily_data['date'].dt.date == date_to_plot]\n",
    "fig = px.line(target_day_data, x='date', y='close', title=f'Price movement on {date_to_plot}')\n",
    "# Adding a shaded region to indicate the dull period\n",
    "start_of_dull_period = target_day_data[target_day_data['dull_period'] == 1].iloc[0]['date'] - timedelta(minutes=90)\n",
    "end_of_dull_period = target_day_data[target_day_data['dull_period'] == 1].iloc[0]['date']\n",
    "fig.add_vrect(x0=start_of_dull_period, x1=end_of_dull_period, fillcolor=\"lightslategrey\", opacity=0.5, line_width=0)\n",
    "\n",
    "# Adding a line to indicate the upper bound\n",
    "upper_bound = target_day_data[target_day_data['dull_period'] == 1].iloc[0]['upper_bound']\n",
    "upper_bound_crossed = target_day_data.set_index('date').loc[end_of_dull_period:, 'high'].max() > upper_bound\n",
    "color = 'green' if upper_bound_crossed else 'white'\n",
    "fig.add_shape(\n",
    "        type=\"line\",\n",
    "        xref=\"x\",\n",
    "        yref=\"y\",\n",
    "        x0=end_of_dull_period, \n",
    "        y0=upper_bound,\n",
    "        x1=target_day_data.iloc[-1]['date'],\n",
    "        y1=upper_bound,\n",
    "        line=dict(color=color, dash=\"dot\"),\n",
    ")\n",
    "fig.add_annotation(\n",
    "        x=target_day_data.iloc[-1]['date'],\n",
    "        y=upper_bound,\n",
    "        text=\"Upper Bound\",\n",
    "        showarrow=False,\n",
    "        yshift=10,\n",
    "        font=dict(color=color)\n",
    ")\n",
    "\n",
    "\n",
    "# Adding a line to indicate the lower bound. The line will start from the end of the dull period on the x-axis\n",
    "lower_bound = target_day_data[target_day_data['dull_period'] == 1].iloc[0]['lower_bound']\n",
    "lower_bound_crossed = target_day_data.set_index('date').loc[end_of_dull_period:, 'low'].min() < lower_bound\n",
    "color = 'green' if lower_bound_crossed else 'white'\n",
    "\n",
    "fig.add_shape(\n",
    "        type=\"line\",\n",
    "        xref=\"x\",\n",
    "        yref=\"y\",\n",
    "        x0=end_of_dull_period, \n",
    "        y0=lower_bound,\n",
    "        x1=target_day_data.iloc[-1]['date'],\n",
    "        y1=lower_bound,\n",
    "        line=dict(color=color, dash=\"dot\"),\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "        x=target_day_data.iloc[-1]['date'],\n",
    "        y=lower_bound,\n",
    "        text=\"Lower Bound\",\n",
    "        showarrow=False,\n",
    "        yshift=10,\n",
    "        font=dict(color=color)\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Working out the best stop loss for intraday trend"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def analyse_stoploss(df, one_min_df, trend_buffer=0.1):\n",
    "\n",
    "    stop_loss_days = df.trade_data.apply(lambda x: x.get('entry_1', {}).get('returns', np.nan)).dropna()\n",
    "    stop_loss_days = stop_loss_days[stop_loss_days <= -0.3]\n",
    "\n",
    "    first_trade_direction = df.trade_data.apply(lambda x: x.get('entry_1', {}).get('trend_direction', np.nan)).dropna()\n",
    "\n",
    "    trend_days = df.open_to_close_trend_abs[df.open_to_close_trend_abs > df.threshold_movement + trend_buffer]\n",
    "    days_with_stoploss_and_trend = stop_loss_days.index.intersection(trend_days.index)\n",
    "    days_with_stoploss_and_trend = df.loc[days_with_stoploss_and_trend]\n",
    "    days_with_stoploss_and_trend = days_with_stoploss_and_trend[['open_vix', 'day_open', 'open_to_close_trend', 'open_to_close_trend_abs', 'upper_bound', 'lower_bound']]\n",
    "\n",
    "    days_with_stoploss_and_trend['first_trade_direction'] = first_trade_direction\n",
    "\n",
    "    days_with_stoploss_and_trend['trend_direction'] = days_with_stoploss_and_trend.open_to_close_trend.apply(lambda x: 1 if x > 0 else -1)\n",
    "\n",
    "    # Calculating the maximum other direction movement for the day\n",
    "    days_with_stoploss_and_trend['extreme_price'] = days_with_stoploss_and_trend.apply(lambda x: one_min_df.loc[x.name.date().strftime('%Y-%m-%d')].close.min() if x.first_trade_direction == 1 else one_min_df.loc[x.name.date().strftime('%Y-%m-%d')].close.max(), axis=1)\n",
    "    days_with_stoploss_and_trend['max_other_direction_movement'] = np.where(days_with_stoploss_and_trend.first_trade_direction == 1, days_with_stoploss_and_trend.upper_bound / days_with_stoploss_and_trend.extreme_price - 1, days_with_stoploss_and_trend.lower_bound / days_with_stoploss_and_trend.extreme_price - 1)\n",
    "    return days_with_stoploss_and_trend\n",
    "\n",
    "analysed_df = analyse_stoploss(trend_bnf, bnf_onemin)\n",
    "analysed_df.max_other_direction_movement.abs().median()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Intraday Trend - Constituent analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_index_with_constituent_trend_data(index_name, trend_df):\n",
    "\n",
    "    index_onemin = pd.read_csv(f'data/{index_name}_onemin_prices.csv', index_col=0, parse_dates=True)\n",
    "    index_daily_open = (index_onemin.groupby(index_onemin.index.date).apply(lambda x: x.iloc[1]).open.to_frame())\n",
    "    index_onemin['day_open'] = index_daily_open.loc[index_onemin.index.date].values\n",
    "    index_onemin['change_from_open'] = index_onemin['close'] / index_onemin['day_open'] - 1\n",
    "    index_onemin = index_onemin[['change_from_open']]\n",
    "    index_onemin.columns = map(lambda x: f'{index_name}_{x}', index_onemin.columns)\n",
    "    tickers, weights = vs.get_index_constituents(index_name)\n",
    "    ticker_dfs = []\n",
    "    for ticker, weight in zip(tickers, weights):\n",
    "        ticker_onemin = pd.read_csv(f'data/{ticker}_onemin_prices.csv', index_col=0, parse_dates=True)\n",
    "        ticker_onemin['weight'] = weight/100\n",
    "        ticker_daily_open = (ticker_onemin.groupby(ticker_onemin.index.date).apply(lambda x: x.iloc[1]).open.to_frame())\n",
    "        ticker_onemin['day_open'] = ticker_daily_open.loc[ticker_onemin.index.date].values\n",
    "        ticker_onemin['change_from_open'] = ticker_onemin['close'] / ticker_onemin['day_open'] - 1\n",
    "        ticker_onemin['weighted_change'] = ticker_onemin['change_from_open'] * ticker_onemin['weight']\n",
    "        ticker_onemin = ticker_onemin[['change_from_open', 'weighted_change']]\n",
    "        ticker_onemin.columns = map(lambda x: f'{ticker}_{x}', ticker_onemin.columns)\n",
    "        ticker_dfs.append(ticker_onemin)\n",
    "    full_df = pd.concat(ticker_dfs, axis=1)\n",
    "    full_df['proxy_index_change'] = full_df.filter(regex='weighted_change').sum(axis=1)\n",
    "    full_df = full_df.merge(index_onemin, left_index=True, right_index=True)\n",
    "    for ticker in tickers:\n",
    "        full_df[f'{ticker}_contribution'] = full_df[f'{ticker}_weighted_change'] / full_df[f'proxy_index_change']\n",
    "        full_df[f'{ticker}_contribution_sq'] = full_df[f'{ticker}_contribution'] ** 2\n",
    "    _trigger_times = [day[entry]['trigger_time'] for day in trend_df.trade_data for entry in day.keys() if entry != 'total_returns']\n",
    "    _returns = [day[entry]['returns'] for day in trend_df.trade_data for entry in day.keys() if entry != 'total_returns']\n",
    "    _trend_at_close = [trend_df.set_index(trend_df.index.date).loc[tt.date()].open_to_close_trend for tt in _trigger_times]\n",
    "    _trigger_returns_trend = pd.DataFrame({'trigger_time': _trigger_times, 'returns': _returns, 'trend_at_close': _trend_at_close})\n",
    "    df_to_ret = full_df.merge(_trigger_returns_trend, left_index=True, right_on='trigger_time')\n",
    "    df_to_ret['sum_of_abs_movement'] = df_to_ret.drop(columns=[f'{index_name}_change_from_open']).filter(regex='change_from_open').abs().sum(axis=1)\n",
    "    df_to_ret['std_of_ratio'] = df_to_ret.drop(columns=[f'{index_name}_change_from_open']).filter(regex='change_from_open').div(df_to_ret['sum_of_abs_movement'], axis=0).std(axis=1)\n",
    "    df_to_ret['std_of_constituents'] = df_to_ret.drop(columns=[f'{index_name}_change_from_open']).filter(regex='change_from_open').std(axis=1)\n",
    "    df_to_ret['hhi_index'] = df_to_ret.filter(regex='contribution_sq').sum(axis=1)\n",
    "\n",
    "\n",
    "    return df_to_ret"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_bnf_consolidated = get_index_with_constituent_trend_data('NIFTY BANK', trend_bnf)\n",
    "trend_bnf_consolidated_post_2021 = trend_bnf_consolidated[trend_bnf_consolidated.trigger_time > datetime(2021, 1, 1)]\n",
    "trend_bnf_consolidated_post_2021"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_nifty_consolidated = get_index_with_constituent_trend_data('NIFTY 50', trend_nifty)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_nifty_consolidated_post_2023 = trend_nifty_consolidated[trend_nifty_consolidated.trigger_time > datetime(2023, 1, 1)]\n",
    "trend_nifty_consolidated_post_2023"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "px.scatter(trend_bnf_consolidated_post_2021, x='std_of_constituents', y='std_of_ratio', hover_data=['trigger_time', 'returns', 'trend_at_close'], color='returns', range_color=[-0.3, 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Index flat vs constituents move"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnf_index_vs_cons =  dm.get_index_vs_constituents_recent_vols('BANKNIFTY', return_all=False, simulate_backtest=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnf_index_vs_cons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Insights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Confirming that there is a certain drift in absolute changes as time frame increases\n",
    "\n",
    "for index_name, daily_df in zip(['NIFTY', 'BANKNIFTY', 'FINNIFTY'], [nifty_data, bnf_data, finnifty_data]):\n",
    "\n",
    "    daily_vol = daily_df.resample('B').ffill().close.pct_change().abs().mean()\n",
    "    weekly_vol = daily_df.resample('W').ffill().close.pct_change().abs().mean()\n",
    "    monthly_vol = daily_df.resample('M').ffill().close.pct_change().abs().mean()\n",
    "    yearly_vol = daily_df.resample('Y').ffill().close.pct_change().abs().mean()\n",
    "\n",
    "    weekly_ratio = weekly_vol / daily_vol\n",
    "    monthly_ratio = monthly_vol / daily_vol\n",
    "    yearly_ratio = yearly_vol / daily_vol\n",
    "\n",
    "    weekly_benchmark = 5**0.5\n",
    "    monthly_benchmark = 21**0.5\n",
    "    yearly_benchmark = 252**0.5\n",
    "\n",
    "    weekly_deviation_from_benchmark = weekly_ratio/weekly_benchmark\n",
    "    monthly_deviation_from_benchmark = monthly_ratio/monthly_benchmark\n",
    "    yearly_deviation_from_benchmark = yearly_ratio/yearly_benchmark\n",
    "\n",
    "    print(f'{index_name}\\nDaily Volatility: {daily_vol:0.3f}\\nWeekly Volatility: {weekly_vol: 0.3f}, Weekly Ratio: {weekly_ratio: 0.3f}, Weekly Benchmark: {weekly_benchmark: 0.3f}\\nMonthly Volatility: {monthly_vol: 0.3f}, Monthly Ratio: {monthly_ratio: 0.3f}, Monthly Benchmark: {monthly_benchmark: 0.3f}\\nYearly Volatility: {yearly_vol: 0.3f}, Yearly Ratio: {yearly_ratio: 0.3f}, Yearly Benchmark: {yearly_benchmark: 0.3f}\\n')\n",
    "\n",
    "    print(f'{index_name}\\nWeekly Deviation from Benchmark: {weekly_deviation_from_benchmark: 0.3f}\\nMonthly Deviation from Benchmark: {monthly_deviation_from_benchmark: 0.3f}\\nYearly Deviation from Benchmark: {yearly_deviation_from_benchmark: 0.3f}\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Confirming whether drift is present in intraday movements\n",
    "\n",
    "for onemindf, index_name in zip([nifty_onemin, bnf_onemin, fin_onemin, midcp_onemin], ['NIFTY', 'BANKNIFTY', 'FINNIFTY', 'MIDCAP']):\n",
    "    print(f'{index_name}\\n')\n",
    "\n",
    "    filtered_index = filter(lambda i: i.time() not in [time(9, 15), time(9, 16), time(15, 30)], onemindf.index)\n",
    "    filtered_index = list(filtered_index)\n",
    "\n",
    "    minute_vol_sd = onemindf.close.pct_change()[filtered_index].std()\n",
    "    minute_vol_abs_change = onemindf.close.pct_change()[filtered_index].abs().mean()\n",
    "\n",
    "\n",
    "    print(f'Minute Volatility SD: {minute_vol_sd}')\n",
    "    print(f'Minute Volatility Absolute Change: {minute_vol_abs_change}')\n",
    "\n",
    "    open_close_std = onemindf.close.groupby(onemindf.index.date).apply(lambda x: (x.iloc[-1] / x.iloc[0] - 1)).std()\n",
    "    open_close_abs_change = onemindf.close.groupby(onemindf.index.date).apply(lambda x: (x.iloc[-1] / x.iloc[0] - 1)).abs().mean()\n",
    "\n",
    "    print(f'Open Close SD: {open_close_std}')\n",
    "    print(f'Open Close Absolute Change: {open_close_abs_change}')\n",
    "\n",
    "    ratio_of_volatility = open_close_std / minute_vol_sd\n",
    "    ratio_of_abs_change = open_close_abs_change / minute_vol_abs_change\n",
    "\n",
    "    print(f'Ratio of Volatility: {ratio_of_volatility}')\n",
    "    print(f'Ratio of Absolute Change: {ratio_of_abs_change}\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_to_test = bnf_onemin.loc['2017']\n",
    "filtered_index = filter(lambda i: i.time() not in [time(9, 15), time(9, 16), time(15, 30)], df_to_test.index)\n",
    "filtered_index = list(filtered_index)\n",
    "#df_to_test.close.pct_change()[filtered_index].std()\n",
    "df_to_test.close.groupby(df_to_test.index.date).apply(lambda x: (x.iloc[-1] / x.iloc[0] - 1)).abs().mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Determining the distribution of one min volatility\n",
    "df = bnf_onemin\n",
    "filtered_index = filter(lambda i: i.time() not in [time(9, 15), time(9, 16), time(15, 30)], df.index)\n",
    "filtered_index = list(filtered_index)\n",
    "filtered_df = df.close.pct_change()[filtered_index]\n",
    "px.histogram(x=filtered_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelling IV surface "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reading the data and dropping the index column\n",
    "data = pd.read_csv('data/vol_surface.csv', index_col=0)\n",
    "data.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding the moneyness feature (ratio of spot price to strike price)\n",
    "data['moneyness'] = data['spot'] / data['strike']\n",
    "\n",
    "# Adding the interaction term between distance squared and time to expiry\n",
    "data['distance_time_interaction'] = data['distance_squared'] * data['time_to_expiry']\n",
    "\n",
    "# Display the first few rows to verify the added features\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Performing one-hot encoding with one column left out (drop_first=True) for the 'symbol' column\n",
    "data_encoded = pd.get_dummies(data, columns=['symbol'], drop_first=True)\n",
    "\n",
    "# Display the first few rows to verify the encoding\n",
    "data_encoded.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Selecting the features\n",
    "features = ['time_to_expiry', 'distance', 'distance_squared', 'moneyness', 'distance_time_interaction', 'symbol_FINNIFTY', 'symbol_NIFTY']\n",
    "X = data_encoded[features]\n",
    "\n",
    "# Selecting the target variable\n",
    "y = data_encoded['iv_multiple']\n",
    "\n",
    "# Splitting the data into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Checking the shapes of the training and validation sets\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Defining the time-to-expiry ranges\n",
    "time_to_expiry_ranges = [(0, 0.0003), (0.0003, 0.0008), (0.0008, (1/365)), ((1/365), (3/365)), ((3/365), (10/365)), ((10/365), 10.0)]\n",
    "\n",
    "# Dictionaries to store the trained models for each segment\n",
    "random_forest_models = {}\n",
    "\n",
    "# Looping through the ranges to train models for each segment\n",
    "for i, (lower_bound, upper_bound) in enumerate(time_to_expiry_ranges):\n",
    "    # Filtering the training data for the current segment\n",
    "    X_train_segment = X_train[(X_train['time_to_expiry'] >= lower_bound) & (X_train['time_to_expiry'] < upper_bound)]\n",
    "    y_train_segment = y_train[X_train_segment.index]\n",
    "    \n",
    "    # Define the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'max_features': [1.0, 'sqrt', 0.5]\n",
    "    }\n",
    "\n",
    "    # Create the Random Forest model\n",
    "    rf_segment = RandomForestRegressor(random_state=42, min_samples_split=5)\n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "    rf_segment.fit(X_train_segment.drop(columns=['time_to_expiry']), y_train_segment)\n",
    "\n",
    "    random_forest_models[(round(lower_bound, 4), round(upper_bound, 4))] = rf_segment\n",
    "\n",
    "\n",
    "# Models trained for all segments\n",
    "\"Models trained successfully for all segments!\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Function to get the appropriate model based on time to expiry\n",
    "def get_model_for_time_to_expiry(time_to_expiry, models):\n",
    "    # Filtering the models based on the time to expiry\n",
    "    filtered_model = [*filter(lambda x: x[0] <= time_to_expiry < x[1], models)][0]\n",
    "    # Returning the model for the segment\n",
    "    return models[filtered_model]\n",
    "\n",
    "# Predicting on the test set using the respective Random Forest model for each sample\n",
    "y_pred_val = np.array([get_model_for_time_to_expiry(time_to_expiry, random_forest_models).predict(pd.DataFrame([features], columns=X_train.columns[1:]))[0] for time_to_expiry, features in zip(X_val['time_to_expiry'], X_val.drop(columns=['time_to_expiry']).values)])\n",
    "\n",
    "\n",
    "# Calculating the mean squared error for the Random Forest models on the test set\n",
    "mse_rf = mean_squared_error(y_val, y_pred_val)\n",
    "mse_rf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the predicted vs actual values and using different colors for different ranges of time to expiry\n",
    "fig = px.scatter(x=y_val, y=y_pred_val, color=X_val['time_to_expiry'], color_continuous_scale='RdBu', range_color=(0, 0.03))\n",
    "fig.update_layout(xaxis_title='Actual', yaxis_title='Predicted')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Comparing the random forest model with the regression model from the volstreet package\n",
    "from volstreet.blackscholes import iv_transformer_coeffs_wip\n",
    "\n",
    "regression_coeffs = X_val.time_to_expiry.apply(iv_transformer_coeffs_wip)\n",
    "regression_coeffs = regression_coeffs.apply(pd.Series)\n",
    "regression_coeffs.columns = ['dis_sq_coeff', 'dis_coeff', 'intercept']\n",
    "regression_coeffs['y_pred_reg'] = regression_coeffs.dis_sq_coeff * X_val['distance_squared'] + regression_coeffs.dis_coeff * X_val['distance'] + regression_coeffs.intercept\n",
    "\n",
    "mse_reg = mean_squared_error(y_val, regression_coeffs['y_pred_reg'])\n",
    "mse_reg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the regression model's predicted vs actual values and using different colors for different ranges of time to expiry\n",
    "fig = px.scatter(x=y_val, y=regression_coeffs['y_pred_reg'], color=X_val['time_to_expiry'], color_continuous_scale='RdBu', range_color=(0, 0.03))\n",
    "fig.update_layout(xaxis_title='Actual', yaxis_title='Predicted')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the vol curve using different colors for different time to expiry for actual and predictions using both models\n",
    "\n",
    "# Scatter plot of distance vs IV multiple - actual values\n",
    "fig = px.scatter(x=X_val['distance'], y=y_val, color=X_val['time_to_expiry'], color_continuous_scale='Blues', range_color=(0, 0.03))\n",
    "fig.update_layout(xaxis_title='Distance', yaxis_title='IV Multiple')\n",
    "\n",
    "# Scatter plot of distance vs IV multiple - predicted values\n",
    "fig2 = px.scatter(x=X_val['distance'], y=y_pred_val, color=X_val['time_to_expiry'], color_continuous_scale='Reds', range_color=(0, 0.03))\n",
    "fig2.update_layout(xaxis_title='Distance', yaxis_title='IV Multiple')\n",
    "\n",
    "# Scatter plot of distance vs IV multiple - regression model's predicted values\n",
    "fig3 = px.scatter(x=X_val['distance'], y=regression_coeffs['y_pred_reg'], color=X_val['time_to_expiry'], color_continuous_scale='Greens', range_color=(0, 0.03))\n",
    "fig3.update_layout(xaxis_title='Distance', yaxis_title='IV Multiple')\n",
    "\n",
    "# Show the plots\n",
    "fig.show()\n",
    "fig2.show()\n",
    "fig3.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Feature importance for the Random Forest model\n",
    "\n",
    "# Create a DataFrame to store the feature importance for all segments\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each segment, extract feature importance, and add to the DataFrame\n",
    "for segment, model in random_forest_models.items():\n",
    "    feature_importance = model.feature_importances_\n",
    "    segment_importance_df = pd.DataFrame({\n",
    "        'Feature': X_train.drop(columns=['time_to_expiry']).columns,\n",
    "        'Importance': feature_importance,\n",
    "        'Segment': [segment] * len(feature_importance)\n",
    "    })\n",
    "    feature_importance_df = pd.concat([feature_importance_df, segment_importance_df])\n",
    "\n",
    "# Create a Plotly figure using the DataFrame\n",
    "fig = px.bar(feature_importance_df, x='Importance', y='Feature', color='Segment',\n",
    "             title='Random Forest Feature Importance by Segment',\n",
    "             labels={'Importance': 'Feature Importance', 'Feature': 'Feature'},\n",
    "             orientation='h',\n",
    "             category_orders={'Segment': sorted(feature_importance_df['Segment'].unique(), reverse=True)})\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Saving the models\n",
    "from joblib import dump\n",
    "\n",
    "for segment, model in random_forest_models.items():\n",
    "    dump(model, f'iv_models/random_forest_model_{segment}.joblib')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading the models\n",
    "from joblib import load\n",
    "import os\n",
    "import ast\n",
    "\n",
    "random_forest_models_loaded = {}\n",
    "for file in os.listdir('iv_models'):\n",
    "    if file.endswith('.joblib'):\n",
    "        str_literal = file.split('_')[-1].rstrip('.joblib')\n",
    "        segment = ast.literal_eval(str_literal)\n",
    "        random_forest_models_loaded[segment] = load(f'iv_models/{file}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelling IV surface old "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modelling IV surface\n",
    "vol_surface = pd.read_csv('data/vol_surface.csv', index_col=0)\n",
    "#vol_surface = vol_surface.drop(vol_surface[vol_surface.isna().all(axis=1)].index)\n",
    "vol_surface['tte'] = vol_surface.time_to_expiry.apply(lambda num: round(num, 4))\n",
    "vol_surface"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modelling IV surface\n",
    "vol_surface_dict = {}\n",
    "for tte in vol_surface.tte.unique():\n",
    "    X = vol_surface.loc[vol_surface.tte == tte][['distance', 'distance_squared']]\n",
    "    y = vol_surface.loc[vol_surface.tte == tte]['iv_multiple']\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    dis_sq_coeff, dis_coeff, intercept = model.coef_[1], model.coef_[0], model.intercept_\n",
    "    score = model.score(X, y)\n",
    "    if score > 0.9:\n",
    "        vol_surface_dict[tte] = {'dis_sq_coeff': dis_sq_coeff, 'dis_coeff': dis_coeff, 'intercept': intercept, 'score': score}\n",
    "    # print(f'{tte} days to expiry: Coefficients: {model.coef_}, Intercept: {model.intercept_}, R2: {model.score(X, y)}')\n",
    "vol_surface_weights = pd.DataFrame(vol_surface_dict).T.reset_index().rename(columns={'index': 'time_to_expiry'})\n",
    "vol_surface_weights.sort_values('time_to_expiry', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vol_surface_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = px.scatter(vol_surface_weights, x='time_to_expiry', y='dis_sq_coeff')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def func(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c\n",
    "lower_bounds = [-np.inf, -np.inf, -np.inf]\n",
    "upper_bounds = [np.inf, np.inf, np.inf]\n",
    "popt, pcov = curve_fit(func, vol_surface_weights['time_to_expiry'], vol_surface_weights['dis_sq_coeff'], bounds=(lower_bounds, upper_bounds))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dummy_range = np.arange(0, 1, 0.0001)\n",
    "fig.add_trace(px.line(x=dummy_range, y=func(dummy_range, *popt)).data[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "popt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modelling IV surface - Distance Squared coefficient vs Time to Expiry (inverse)\n",
    "for param in np.arange(0.02, 1.5, 0.01):\n",
    "    vol_surface_weights['tte_inverse'] = 1 / (vol_surface_weights.time_to_expiry**param)\n",
    "    X = vol_surface_weights['tte_inverse'].values.reshape(-1, 1)\n",
    "    y = vol_surface_weights['dis_coeff']\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    print(f'{param} param: Coefficients: {model.coef_}, Intercept: {model.intercept_}, R2: {model.score(X, y)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "px.scatter(vol_surface_weights, x='time_to_expiry', y='dis_coeff')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def coefficients_for_surface(tte):\n",
    "\n",
    "    # distance squared coefficient\n",
    "    dfs2 = 3270.27*np.exp(-384.38*tte) + 100\n",
    "    dfs2 = min(dfs2, 20000)\n",
    "\n",
    "    # distance coefficient\n",
    "    if tte < 0.26/365:\n",
    "        dfs = 1\n",
    "    else:\n",
    "        dfs = 1 / ((tte ** 0.45) * 5)\n",
    "        dfs = min(dfs, 5)\n",
    "        dfs = -6 + dfs\n",
    "\n",
    "    # intercept\n",
    "    if tte<3/(24*365):\n",
    "        intercept=1.07\n",
    "    elif tte<0.27/365:\n",
    "        intercept=1\n",
    "    else:\n",
    "        intercept=0.98\n",
    "    return dfs2, dfs, intercept"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "coefficients_for_surface(2/365)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gamblers Fallacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import logging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gambler_logger = logging.getLogger('gambler')\n",
    "gambler_logger.setLevel(logging.INFO)\n",
    "\n",
    "# Setting up the file handler\n",
    "file_handler = logging.FileHandler('gambler_calculations.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Adding the handlers to the logger\n",
    "gambler_logger.addHandler(file_handler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def simulate_roulette_spins(\n",
    "        n_spins, initial_money=None, drought_threshold=150, bet_amount=None, stop_loss=0\n",
    "):\n",
    "\n",
    "    # Initialize droughts for all numbers to 0\n",
    "    droughts = {i: [] for i in range(37)}\n",
    "\n",
    "    # Initialize current drought counts for all numbers to 0\n",
    "    current_droughts = {i: 0 for i in range(37)}\n",
    "\n",
    "    # Initialize money\n",
    "    money = 100 if initial_money is None else initial_money\n",
    "    money_history = [money]\n",
    "    chances_of_winning = 1/35\n",
    "    chances_of_losing = 1 - chances_of_winning\n",
    "    payout = 35\n",
    "    kelly_percentage = chances_of_winning - (chances_of_losing/payout)\n",
    "    bet_amount =  kelly_percentage * money if bet_amount is None else bet_amount * money\n",
    "\n",
    "    stop_loss = drought_threshold + stop_loss\n",
    "\n",
    "    # Simulate n spins\n",
    "    for _ in range(n_spins):\n",
    "        gambler_logger.info(f'Spin number: {_}')\n",
    "        spin_result = random.randint(0, 36)\n",
    "        gambler_logger.info(f'Spin result: {spin_result}')\n",
    "\n",
    "        # Determine which numbers to bet on\n",
    "        numbers_to_bet_on = {number: dryness for number, dryness in current_droughts.items() if dryness > drought_threshold}\n",
    "        gambler_logger.info(f'Numbers to bet on and their droughts: {numbers_to_bet_on}')\n",
    "        numbers_to_bet_on_filtered = {number: dryness for number, dryness in numbers_to_bet_on.items() if dryness < stop_loss}\n",
    "\n",
    "        if numbers_to_bet_on != numbers_to_bet_on_filtered:\n",
    "            gambler_logger.info(f'Numbers filtered down to {numbers_to_bet_on_filtered}')\n",
    "\n",
    "        # Determine how much to bet in total\n",
    "        total_bet_amount = bet_amount * len(numbers_to_bet_on_filtered)\n",
    "        gambler_logger.info(f'Total bet amount: {total_bet_amount}')\n",
    "\n",
    "        # Update money based on bet results\n",
    "        if spin_result in numbers_to_bet_on_filtered:\n",
    "            # If one of the numbers you bet on came up, you win 36 times the bet amount for that number\n",
    "            money += 36*bet_amount - total_bet_amount\n",
    "            gambler_logger.info(f'Won {36*bet_amount - total_bet_amount}')\n",
    "        else:\n",
    "            # If none of the numbers you bet on came up, you lose the total bet amount\n",
    "            money -= total_bet_amount\n",
    "            gambler_logger.info(f'Lost {total_bet_amount}')\n",
    "\n",
    "        # Update money history\n",
    "        money_history.append(money)\n",
    "        gambler_logger.info(f'Money: {money}')\n",
    "\n",
    "        # Update drought counts\n",
    "        for number in range(37):\n",
    "            if number == spin_result:\n",
    "                # If the number came up, reset its drought count and append the drought to the list\n",
    "                droughts.get(number, []).append(current_droughts[number])\n",
    "                current_droughts[number] = 0\n",
    "                gambler_logger.info(f'Number {number} came up. Resetting drought count.')\n",
    "            else:\n",
    "                # If the number didn't come up, increment its drought count\n",
    "                current_droughts[number] += 1\n",
    "\n",
    "    return droughts, money_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of spins\n",
    "monies = []\n",
    "spins = 10000\n",
    "\n",
    "for _ in range(25):\n",
    "    droughts, money_history = simulate_roulette_spins(spins, initial_money=100, drought_threshold=100, stop_loss=5)\n",
    "    monies.append(money_history[-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the monies\n",
    "px.histogram(x=monies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the money history\n",
    "px.line(y=money_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_droughts = []\n",
    "for drought in droughts.values():\n",
    "    all_droughts.extend(drought)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the droughts\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=all_droughts, nbinsx=20, cumulative=dict(enabled=True), histnorm='probability')\n",
    ")\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max(all_droughts)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
