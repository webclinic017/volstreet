{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import volstreet.datamodule as dm\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from datetime import time, datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import volstreet as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using DataClient class\n",
    "client = dm.DataClient(api_key=__import__('os').environ['EOD_API_KEY'])\n",
    "kite_user = __import__('os').environ['KITE_USER']\n",
    "kite_pass = __import__('os').environ['KITE_PASS']\n",
    "kite_api_key = __import__('os').environ['KITE_API_KEY']\n",
    "kite_api_secret = __import__('os').environ['KITE_API_SECRET']\n",
    "kite_auth_key = __import__('os').environ['KITE_AUTH_KEY']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using get_data and analyser functions\n",
    "nifty_data = client.get_data(symbol='NIFTY')\n",
    "bnf_data = client.get_data(symbol='BANKNIFTY')\n",
    "finnifty_data = client.get_data(symbol='FINNIFTY')\n",
    "nifty_daily_data = dm.analyser(nifty_data, frequency='D')\n",
    "bnf_daily_data = dm.analyser(bnf_data, frequency='D')\n",
    "nifty_weekly_data = dm.analyser(nifty_data, frequency='W-THU')\n",
    "bnf_weekly_data = dm.analyser(bnf_data, frequency='W-THU')\n",
    "nifty_monthly_data = dm.analyser(nifty_data, frequency='M-THU')\n",
    "bnf_monthly_data = dm.analyser(bnf_data, frequency='M-THU')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using ratio_analysis function\n",
    "rolling_periods = 5\n",
    "ratio_data = dm.ratio_analysis(bnf_weekly_data, nifty_weekly_data, add_rolling=rolling_periods)\n",
    "px.line(ratio_data, x=ratio_data.index, y=['BANKNIFTY Change', 'NIFTY Change', f'Rolling {rolling_periods} Ratio'], color_discrete_map={'BANKNIFTY Change': 'red', 'NIFTY Change': 'blue', f'Rolling {rolling_periods} Ratio': 'green'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ratio_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using gambler function for NIFTY and BANKNIFTY\n",
    "for index in ['NIFTY', 'BANKNIFTY']:\n",
    "    print(f'{index}\\n')\n",
    "    data = client.get_data(symbol=index)\n",
    "    for frequency in ['D', 'D-THU', 'W-THU', 'M-THU']:\n",
    "        print(f'{frequency}\\n')\n",
    "        dm.gambler(data, frequency, 'abs_change')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using gambler function for FINNIFTY\n",
    "for index in ['FINNIFTY']:\n",
    "    print(f'{index}\\n')\n",
    "    data = client.get_data(symbol=index)\n",
    "    for frequency in ['D', 'D-TUE', 'W-TUE', 'M-TUE']:\n",
    "        print(f'{frequency}\\n')\n",
    "        dm.gambler(data, frequency, 'abs_change')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Rolling average of absolute change to support gambler function\n",
    "analysed_df = dm.analyser(finnifty_data, frequency='W-tue')\n",
    "rolling_periods = 9\n",
    "analysed_df['rolling'] = analysed_df['abs_change'].rolling(rolling_periods, min_periods=1).mean()\n",
    "px.line(analysed_df, x=analysed_df.index, y='rolling')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analysed_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# One min data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kite_obj = dm.get_greenlit_kite(kite_api_key, kite_api_secret, kite_user, kite_pass, kite_auth_key)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Updating one min data for NIFTY 50, NIFTY BANK and NIFTY FIN SERVICE\n",
    "dm.get_1m_data(kite_obj, 'NIFTY 50', path='data\\\\')\n",
    "dm.get_1m_data(kite_obj, 'NIFTY BANK', path='data\\\\')\n",
    "dm.get_1m_data(kite_obj, 'NIFTY FIN SERVICE', path='data\\\\')\n",
    "dm.get_1m_data(kite_obj, 'NIFTY MID SELECT', path='data\\\\')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dm.get_constituent_1m_data(kite_obj, 'NIFTY', path='data\\\\')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Intraday Trend"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nifty_onemin = pd.read_csv('data/NIFTY 50_onemin_prices.csv', index_col=0, parse_dates=True)\n",
    "bnf_onemin = pd.read_csv('data/NIFTY BANK_onemin_prices.csv', index_col=0, parse_dates=True)\n",
    "fin_onemin = pd.read_csv('data/NIFTY FIN SERVICE_onemin_prices.csv', index_col=0, parse_dates=True)\n",
    "midcp_onemin = pd.read_csv('data/NIFTY MID SELECT_onemin_prices.csv', index_col=0, parse_dates=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_nifty = dm.backtest_intraday_trend(nifty_onemin, open_nth = 0, beta = 0.8, eod_client=client, max_entries=3, rolling_days=90, stop_loss=0.3)\n",
    "trend_bnf = dm.backtest_intraday_trend(bnf_onemin, open_nth = 0, beta = 0.8, eod_client=client, max_entries=3, rolling_days=90, stop_loss=0.3)\n",
    "trend_finnifty = dm.backtest_intraday_trend(fin_onemin, open_nth = 0, beta = 0.8, eod_client=client, max_entries=3, rolling_days=90, stop_loss=0.3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_bnf[['threshold_movement', 'total_returns', 'open_to_close_trend_abs']].sum()\n",
    "trend_bnf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the distribution of returns for various entries\n",
    "df_to_plot = trend_finnifty\n",
    "returns_1 = df_to_plot.trade_data.apply(lambda x: x.get('entry_1', {}).get('returns', np.nan)).dropna()\n",
    "returns_2 = df_to_plot.trade_data.apply(lambda x: x.get('entry_2', {}).get('returns', np.nan)).dropna()\n",
    "returns_3 = df_to_plot.trade_data.apply(lambda x: x.get('entry_3', {}).get('returns', np.nan)).dropna()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=returns_1, name='Entry 1', nbinsx=10))\n",
    "fig.add_trace(go.Histogram(x=returns_2, name='Entry 2', nbinsx=10))\n",
    "fig.add_trace(go.Histogram(x=returns_3, name='Entry 3', nbinsx=10))\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Year wise summary of returns\n",
    "df_to_sum = trend_nifty\n",
    "df_to_sum.groupby(df_to_sum.index.year).total_returns.sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_indices_returns = trend_bnf.merge(trend_finnifty, left_index=True, right_index=True, suffixes=('_bnf', '_finnifty')).merge(trend_nifty, left_index=True, right_index=True, suffixes=('', '_nifty'))[['total_returns', 'total_returns_bnf', 'total_returns_finnifty']]\n",
    "all_indices_returns.sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the distribution of the ratio for stop loss and no stop loss days for all indices\n",
    "all_indices_with_drivers = pd.concat([trend_nifty, trend_bnf, trend_finnifty])\n",
    "all_indices_with_drivers_stop_loss =  all_indices_with_drivers[(all_indices_with_drivers.total_returns <= 0)]\n",
    "all_indices_with_drivers_no_stop_loss =  all_indices_with_drivers[(all_indices_with_drivers.total_returns > 0)]\n",
    "fig = px.histogram(all_indices_with_drivers_stop_loss, x='ratio')\n",
    "fig.add_trace(go.Histogram(x=all_indices_with_drivers_no_stop_loss.ratio, name='No Stop Loss'))\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting the minute vol and open to close trend on different y axis\n",
    "df_to_plot = trend_finnifty\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(go.Line(x=df_to_plot.index, y=df_to_plot['rolling_ratio'], name='Ratio'), secondary_y=True)\n",
    "fig.add_trace(go.Line(x=df_to_plot.index, y=df_to_plot['strat_nav'], name='Nav'), secondary_y=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trying different beta values\n",
    "for beta in range(80, 105, 5):\n",
    "    beta = beta/100\n",
    "    trend_nifty = dm.backtest_intraday_trend(nifty_onemin, beta = beta, eod_client=client)\n",
    "    print(f'Beta: {beta}')\n",
    "    print(f'NIFTY: {trend_nifty[\"total_returns\"].sum()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Working out the best stop loss for intraday trend"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def analyse_stoploss(df, one_min_df, trend_buffer=0.1):\n",
    "\n",
    "    stop_loss_days = df.trade_data.apply(lambda x: x.get('entry_1', {}).get('returns', np.nan)).dropna()\n",
    "    stop_loss_days = stop_loss_days[stop_loss_days <= -0.3]\n",
    "\n",
    "    first_trade_direction = df.trade_data.apply(lambda x: x.get('entry_1', {}).get('trend_direction', np.nan)).dropna()\n",
    "\n",
    "    trend_days = df.open_to_close_trend_abs[df.open_to_close_trend_abs > df.threshold_movement + trend_buffer]\n",
    "    days_with_stoploss_and_trend = stop_loss_days.index.intersection(trend_days.index)\n",
    "    days_with_stoploss_and_trend = df.loc[days_with_stoploss_and_trend]\n",
    "    days_with_stoploss_and_trend = days_with_stoploss_and_trend[['open_vix', 'day_open', 'open_to_close_trend', 'open_to_close_trend_abs', 'upper_bound', 'lower_bound']]\n",
    "\n",
    "    days_with_stoploss_and_trend['first_trade_direction'] = first_trade_direction\n",
    "\n",
    "    days_with_stoploss_and_trend['trend_direction'] = days_with_stoploss_and_trend.open_to_close_trend.apply(lambda x: 1 if x > 0 else -1)\n",
    "\n",
    "    # Calculating the maximum other direction movement for the day\n",
    "    days_with_stoploss_and_trend['extreme_price'] = days_with_stoploss_and_trend.apply(lambda x: one_min_df.loc[x.name.date().strftime('%Y-%m-%d')].close.min() if x.first_trade_direction == 1 else one_min_df.loc[x.name.date().strftime('%Y-%m-%d')].close.max(), axis=1)\n",
    "    days_with_stoploss_and_trend['max_other_direction_movement'] = np.where(days_with_stoploss_and_trend.first_trade_direction == 1, days_with_stoploss_and_trend.upper_bound / days_with_stoploss_and_trend.extreme_price - 1, days_with_stoploss_and_trend.lower_bound / days_with_stoploss_and_trend.extreme_price - 1)\n",
    "    return days_with_stoploss_and_trend\n",
    "\n",
    "analysed_df = analyse_stoploss(trend_bnf, bnf_onemin)\n",
    "analysed_df.max_other_direction_movement.abs().median()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Intraday Trend - Constituent analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_index_with_constituent_trend_data(index_name, trend_df):\n",
    "\n",
    "    index_onemin = pd.read_csv(f'data/{index_name}_onemin_prices.csv', index_col=0, parse_dates=True)\n",
    "    index_daily_open = (index_onemin.groupby(index_onemin.index.date).apply(lambda x: x.iloc[1]).open.to_frame())\n",
    "    index_onemin['day_open'] = index_daily_open.loc[index_onemin.index.date].values\n",
    "    index_onemin['change_from_open'] = index_onemin['close'] / index_onemin['day_open'] - 1\n",
    "    index_onemin = index_onemin[['change_from_open']]\n",
    "    index_onemin.columns = map(lambda x: f'{index_name}_{x}', index_onemin.columns)\n",
    "    tickers, weights = vs.get_index_constituents(index_name)\n",
    "    ticker_dfs = []\n",
    "    for ticker, weight in zip(tickers, weights):\n",
    "        ticker_onemin = pd.read_csv(f'data/{ticker}_onemin_prices.csv', index_col=0, parse_dates=True)\n",
    "        ticker_onemin['weight'] = weight/100\n",
    "        ticker_daily_open = (ticker_onemin.groupby(ticker_onemin.index.date).apply(lambda x: x.iloc[1]).open.to_frame())\n",
    "        ticker_onemin['day_open'] = ticker_daily_open.loc[ticker_onemin.index.date].values\n",
    "        ticker_onemin['change_from_open'] = ticker_onemin['close'] / ticker_onemin['day_open'] - 1\n",
    "        ticker_onemin['weighted_change'] = ticker_onemin['change_from_open'] * ticker_onemin['weight']\n",
    "        ticker_onemin = ticker_onemin[['change_from_open', 'weighted_change']]\n",
    "        ticker_onemin.columns = map(lambda x: f'{ticker}_{x}', ticker_onemin.columns)\n",
    "        ticker_dfs.append(ticker_onemin)\n",
    "    full_df = pd.concat(ticker_dfs, axis=1)\n",
    "    full_df['proxy_index_change'] = full_df.filter(regex='weighted_change').sum(axis=1)\n",
    "    full_df = full_df.merge(index_onemin, left_index=True, right_index=True)\n",
    "    for ticker in tickers:\n",
    "        full_df[f'{ticker}_contribution'] = full_df[f'{ticker}_weighted_change'] / full_df[f'proxy_index_change']\n",
    "        full_df[f'{ticker}_contribution_sq'] = full_df[f'{ticker}_contribution'] ** 2\n",
    "    _trigger_times = [day[entry]['trigger_time'] for day in trend_df.trade_data for entry in day.keys() if entry != 'total_returns']\n",
    "    _returns = [day[entry]['returns'] for day in trend_df.trade_data for entry in day.keys() if entry != 'total_returns']\n",
    "    _trend_at_close = [trend_df.set_index(trend_df.index.date).loc[tt.date()].open_to_close_trend for tt in _trigger_times]\n",
    "    _trigger_returns_trend = pd.DataFrame({'trigger_time': _trigger_times, 'returns': _returns, 'trend_at_close': _trend_at_close})\n",
    "    df_to_ret = full_df.merge(_trigger_returns_trend, left_index=True, right_on='trigger_time')\n",
    "    df_to_ret['sum_of_abs_movement'] = df_to_ret.drop(columns=[f'{index_name}_change_from_open']).filter(regex='change_from_open').abs().sum(axis=1)\n",
    "    df_to_ret['std_of_ratio'] = df_to_ret.drop(columns=[f'{index_name}_change_from_open']).filter(regex='change_from_open').div(df_to_ret['sum_of_abs_movement'], axis=0).std(axis=1)\n",
    "    df_to_ret['std_of_constituents'] = df_to_ret.drop(columns=[f'{index_name}_change_from_open']).filter(regex='change_from_open').std(axis=1)\n",
    "    df_to_ret['hhi_index'] = df_to_ret.filter(regex='contribution_sq').sum(axis=1)\n",
    "\n",
    "\n",
    "    return df_to_ret"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_bnf_consolidated = get_index_with_constituent_trend_data('NIFTY BANK', trend_bnf)\n",
    "trend_bnf_consolidated_post_2021 = trend_bnf_consolidated[trend_bnf_consolidated.trigger_time > datetime(2021, 1, 1)]\n",
    "trend_bnf_consolidated_post_2021"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_nifty_consolidated = get_index_with_constituent_trend_data('NIFTY 50', trend_nifty)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_nifty_consolidated_post_2023 = trend_nifty_consolidated[trend_nifty_consolidated.trigger_time > datetime(2023, 1, 1)]\n",
    "trend_nifty_consolidated_post_2023"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "px.scatter(trend_bnf_consolidated_post_2021, x='std_of_constituents', y='std_of_ratio', hover_data=['trigger_time', 'returns', 'trend_at_close'], color='returns', range_color=[-0.3, 1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Index flat vs constituents move"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnf_index_vs_cons =  dm.get_index_vs_constituents_recent_vols('BANKNIFTY', return_all=False, simulate_backtest=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bnf_index_vs_cons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Insights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Confirming that there is a certain drift in absolute changes as time frame increases\n",
    "\n",
    "for index_name, daily_df in zip(['NIFTY', 'BANKNIFTY', 'FINNIFTY'], [nifty_data, bnf_data, finnifty_data]):\n",
    "\n",
    "    daily_vol = daily_df.resample('B').ffill().close.pct_change().abs().mean()\n",
    "    weekly_vol = daily_df.resample('W').ffill().close.pct_change().abs().mean()\n",
    "    monthly_vol = daily_df.resample('M').ffill().close.pct_change().abs().mean()\n",
    "    yearly_vol = daily_df.resample('Y').ffill().close.pct_change().abs().mean()\n",
    "\n",
    "    weekly_ratio = weekly_vol / daily_vol\n",
    "    monthly_ratio = monthly_vol / daily_vol\n",
    "    yearly_ratio = yearly_vol / daily_vol\n",
    "\n",
    "    weekly_benchmark = 5**0.5\n",
    "    monthly_benchmark = 21**0.5\n",
    "    yearly_benchmark = 252**0.5\n",
    "\n",
    "    weekly_deviation_from_benchmark = weekly_ratio/weekly_benchmark\n",
    "    monthly_deviation_from_benchmark = monthly_ratio/monthly_benchmark\n",
    "    yearly_deviation_from_benchmark = yearly_ratio/yearly_benchmark\n",
    "\n",
    "    print(f'{index_name}\\nDaily Volatility: {daily_vol:0.3f}\\nWeekly Volatility: {weekly_vol: 0.3f}, Weekly Ratio: {weekly_ratio: 0.3f}, Weekly Benchmark: {weekly_benchmark: 0.3f}\\nMonthly Volatility: {monthly_vol: 0.3f}, Monthly Ratio: {monthly_ratio: 0.3f}, Monthly Benchmark: {monthly_benchmark: 0.3f}\\nYearly Volatility: {yearly_vol: 0.3f}, Yearly Ratio: {yearly_ratio: 0.3f}, Yearly Benchmark: {yearly_benchmark: 0.3f}\\n')\n",
    "\n",
    "    print(f'{index_name}\\nWeekly Deviation from Benchmark: {weekly_deviation_from_benchmark: 0.3f}\\nMonthly Deviation from Benchmark: {monthly_deviation_from_benchmark: 0.3f}\\nYearly Deviation from Benchmark: {yearly_deviation_from_benchmark: 0.3f}\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Confirming whether drift is present in intraday movements\n",
    "\n",
    "for onemindf, index_name in zip([nifty_onemin, bnf_onemin, fin_onemin, midcp_onemin], ['NIFTY', 'BANKNIFTY', 'FINNIFTY', 'MIDCAP']):\n",
    "    print(f'{index_name}\\n')\n",
    "\n",
    "    filtered_index = filter(lambda i: i.time() not in [time(9, 15), time(9, 16), time(15, 30)], onemindf.index)\n",
    "    filtered_index = list(filtered_index)\n",
    "\n",
    "    minute_vol_sd = onemindf.close.pct_change()[filtered_index].std()\n",
    "    minute_vol_abs_change = onemindf.close.pct_change()[filtered_index].abs().mean()\n",
    "\n",
    "\n",
    "    print(f'Minute Volatility SD: {minute_vol_sd}')\n",
    "    print(f'Minute Volatility Absolute Change: {minute_vol_abs_change}')\n",
    "\n",
    "    open_close_std = onemindf.close.groupby(onemindf.index.date).apply(lambda x: (x.iloc[-1] / x.iloc[0] - 1)).std()\n",
    "    open_close_abs_change = onemindf.close.groupby(onemindf.index.date).apply(lambda x: (x.iloc[-1] / x.iloc[0] - 1)).abs().mean()\n",
    "\n",
    "    print(f'Open Close SD: {open_close_std}')\n",
    "    print(f'Open Close Absolute Change: {open_close_abs_change}')\n",
    "\n",
    "    ratio_of_volatility = open_close_std / minute_vol_sd\n",
    "    ratio_of_abs_change = open_close_abs_change / minute_vol_abs_change\n",
    "\n",
    "    print(f'Ratio of Volatility: {ratio_of_volatility}')\n",
    "    print(f'Ratio of Absolute Change: {ratio_of_abs_change}\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_to_test = bnf_onemin.loc['2017']\n",
    "filtered_index = filter(lambda i: i.time() not in [time(9, 15), time(9, 16), time(15, 30)], df_to_test.index)\n",
    "filtered_index = list(filtered_index)\n",
    "#df_to_test.close.pct_change()[filtered_index].std()\n",
    "df_to_test.close.groupby(df_to_test.index.date).apply(lambda x: (x.iloc[-1] / x.iloc[0] - 1)).abs().mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Determining the distribution of one min volatility\n",
    "df = bnf_onemin\n",
    "filtered_index = filter(lambda i: i.time() not in [time(9, 15), time(9, 16), time(15, 30)], df.index)\n",
    "filtered_index = list(filtered_index)\n",
    "filtered_df = df.close.pct_change()[filtered_index]\n",
    "px.histogram(x=filtered_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelling IV surface"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modelling IV surface\n",
    "vol_surface = pd.read_csv('data/vol_surface.csv', index_col=0)\n",
    "#vol_surface = vol_surface.drop(vol_surface[vol_surface.isna().all(axis=1)].index)\n",
    "vol_surface['tte'] = vol_surface.time_to_expiry.apply(lambda num: round(num, 4))\n",
    "vol_surface"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modelling IV surface\n",
    "vol_surface_dict = {}\n",
    "for tte in vol_surface.tte.unique():\n",
    "    X = vol_surface.loc[vol_surface.tte == tte][['distance', 'distance_squared']]\n",
    "    y = vol_surface.loc[vol_surface.tte == tte]['iv_multiple']\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    dis_sq_coeff, dis_coeff, intercept = model.coef_[1], model.coef_[0], model.intercept_\n",
    "    score = model.score(X, y)\n",
    "    if score > 0.9:\n",
    "        vol_surface_dict[tte] = {'dis_sq_coeff': dis_sq_coeff, 'dis_coeff': dis_coeff, 'intercept': intercept, 'score': score}\n",
    "    # print(f'{tte} days to expiry: Coefficients: {model.coef_}, Intercept: {model.intercept_}, R2: {model.score(X, y)}')\n",
    "vol_surface_weights = pd.DataFrame(vol_surface_dict).T.reset_index().rename(columns={'index': 'time_to_expiry'})\n",
    "vol_surface_weights.sort_values('time_to_expiry', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vol_surface_weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = px.scatter(vol_surface_weights, x='time_to_expiry', y='dis_sq_coeff')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def func(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c\n",
    "lower_bounds = [-np.inf, -np.inf, -np.inf]\n",
    "upper_bounds = [np.inf, np.inf, np.inf]\n",
    "popt, pcov = curve_fit(func, vol_surface_weights['time_to_expiry'], vol_surface_weights['dis_sq_coeff'], bounds=(lower_bounds, upper_bounds))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dummy_range = np.arange(0, 1, 0.0001)\n",
    "fig.add_trace(px.line(x=dummy_range, y=func(dummy_range, *popt)).data[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "popt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modelling IV surface - Distance Squared coefficient vs Time to Expiry (inverse)\n",
    "for param in np.arange(0.02, 1.5, 0.01):\n",
    "    vol_surface_weights['tte_inverse'] = 1 / (vol_surface_weights.time_to_expiry**param)\n",
    "    X = vol_surface_weights['tte_inverse'].values.reshape(-1, 1)\n",
    "    y = vol_surface_weights['dis_coeff']\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    print(f'{param} param: Coefficients: {model.coef_}, Intercept: {model.intercept_}, R2: {model.score(X, y)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "px.scatter(vol_surface_weights, x='time_to_expiry', y='dis_coeff')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def coefficients_for_surface(tte):\n",
    "\n",
    "    # distance squared coefficient\n",
    "    dfs2 = 3270.27*np.exp(-384.38*tte) + 100\n",
    "    dfs2 = min(dfs2, 20000)\n",
    "\n",
    "    # distance coefficient\n",
    "    if tte < 0.26/365:\n",
    "        dfs = 1\n",
    "    else:\n",
    "        dfs = 1 / ((tte ** 0.45) * 5)\n",
    "        dfs = min(dfs, 5)\n",
    "        dfs = -6 + dfs\n",
    "\n",
    "    # intercept\n",
    "    if tte<3/(24*365):\n",
    "        intercept=1.07\n",
    "    elif tte<0.27/365:\n",
    "        intercept=1\n",
    "    else:\n",
    "        intercept=0.98\n",
    "    return dfs2, dfs, intercept"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "coefficients_for_surface(2/365)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gamblers Fallacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import logging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gambler_logger = logging.getLogger('gambler')\n",
    "gambler_logger.setLevel(logging.INFO)\n",
    "\n",
    "# Setting up the file handler\n",
    "file_handler = logging.FileHandler('gambler_calculations.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Adding the handlers to the logger\n",
    "gambler_logger.addHandler(file_handler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def simulate_roulette_spins(\n",
    "        n_spins, initial_money=None, drought_threshold=150, bet_amount=None, stop_loss=0\n",
    "):\n",
    "\n",
    "    # Initialize droughts for all numbers to 0\n",
    "    droughts = {i: [] for i in range(37)}\n",
    "\n",
    "    # Initialize current drought counts for all numbers to 0\n",
    "    current_droughts = {i: 0 for i in range(37)}\n",
    "\n",
    "    # Initialize money\n",
    "    money = 100 if initial_money is None else initial_money\n",
    "    money_history = [money]\n",
    "    chances_of_winning = 1/35\n",
    "    chances_of_losing = 1 - chances_of_winning\n",
    "    payout = 35\n",
    "    kelly_percentage = chances_of_winning - (chances_of_losing/payout)\n",
    "    bet_amount =  kelly_percentage * money if bet_amount is None else bet_amount * money\n",
    "\n",
    "    stop_loss = drought_threshold + stop_loss\n",
    "\n",
    "    # Simulate n spins\n",
    "    for _ in range(n_spins):\n",
    "        gambler_logger.info(f'Spin number: {_}')\n",
    "        spin_result = random.randint(0, 36)\n",
    "        gambler_logger.info(f'Spin result: {spin_result}')\n",
    "\n",
    "        # Determine which numbers to bet on\n",
    "        numbers_to_bet_on = {number: dryness for number, dryness in current_droughts.items() if dryness > drought_threshold}\n",
    "        gambler_logger.info(f'Numbers to bet on and their droughts: {numbers_to_bet_on}')\n",
    "        numbers_to_bet_on_filtered = {number: dryness for number, dryness in numbers_to_bet_on.items() if dryness < stop_loss}\n",
    "\n",
    "        if numbers_to_bet_on != numbers_to_bet_on_filtered:\n",
    "            gambler_logger.info(f'Numbers filtered down to {numbers_to_bet_on_filtered}')\n",
    "\n",
    "        # Determine how much to bet in total\n",
    "        total_bet_amount = bet_amount * len(numbers_to_bet_on_filtered)\n",
    "        gambler_logger.info(f'Total bet amount: {total_bet_amount}')\n",
    "\n",
    "        # Update money based on bet results\n",
    "        if spin_result in numbers_to_bet_on_filtered:\n",
    "            # If one of the numbers you bet on came up, you win 36 times the bet amount for that number\n",
    "            money += 36*bet_amount - total_bet_amount\n",
    "            gambler_logger.info(f'Won {36*bet_amount - total_bet_amount}')\n",
    "        else:\n",
    "            # If none of the numbers you bet on came up, you lose the total bet amount\n",
    "            money -= total_bet_amount\n",
    "            gambler_logger.info(f'Lost {total_bet_amount}')\n",
    "\n",
    "        # Update money history\n",
    "        money_history.append(money)\n",
    "        gambler_logger.info(f'Money: {money}')\n",
    "\n",
    "        # Update drought counts\n",
    "        for number in range(37):\n",
    "            if number == spin_result:\n",
    "                # If the number came up, reset its drought count and append the drought to the list\n",
    "                droughts.get(number, []).append(current_droughts[number])\n",
    "                current_droughts[number] = 0\n",
    "                gambler_logger.info(f'Number {number} came up. Resetting drought count.')\n",
    "            else:\n",
    "                # If the number didn't come up, increment its drought count\n",
    "                current_droughts[number] += 1\n",
    "\n",
    "    return droughts, money_history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of spins\n",
    "monies = []\n",
    "spins = 10000\n",
    "\n",
    "for _ in range(25):\n",
    "    droughts, money_history = simulate_roulette_spins(spins, initial_money=100, drought_threshold=100, stop_loss=5)\n",
    "    monies.append(money_history[-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the monies\n",
    "px.histogram(x=monies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the money history\n",
    "px.line(y=money_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_droughts = []\n",
    "for drought in droughts.values():\n",
    "    all_droughts.extend(drought)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the droughts\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=all_droughts, nbinsx=20, cumulative=dict(enabled=True), histnorm='probability')\n",
    ")\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max(all_droughts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def backtest_intraday_trend(\n",
    "    one_min_df,\n",
    "    open_nth=0,\n",
    "    beta=1,\n",
    "    trend_threshold=1,\n",
    "    stop_loss=0.3,\n",
    "    max_entries=3,\n",
    "    eod_client=None,\n",
    "    rolling_days=60,\n",
    "):\n",
    "    one_min_df = one_min_df.copy()\n",
    "    if one_min_df.index.name == \"date\":\n",
    "        one_min_df = one_min_df.reset_index()\n",
    "    one_min_df = one_min_df[\n",
    "        (one_min_df[\"date\"].dt.time > time(9, 15))\n",
    "        & (one_min_df[\"date\"].dt.time < time(15, 30))\n",
    "    ]\n",
    "\n",
    "    unavailable_dates = [\n",
    "        datetime(2015, 2, 28).date(),\n",
    "        datetime(2016, 10, 30).date(),\n",
    "        datetime(2019, 10, 27).date(),\n",
    "        datetime(2020, 2, 1).date(),\n",
    "        datetime(2020, 11, 14).date(),\n",
    "    ]\n",
    "\n",
    "    # Fetching vix data and calculating beta\n",
    "    if eod_client is None:\n",
    "        client = dm.DataClient(api_key=__import__(\"os\").environ.get(\"EOD_API_KEY\"))\n",
    "    else:\n",
    "        client = eod_client\n",
    "\n",
    "    vix = client.get_data(\"VIX\", return_columns=[\"open\", \"close\"])\n",
    "    vix = vix.resample(\"B\").ffill()\n",
    "    vix[\"open\"] = vix[\"open\"] * beta\n",
    "    vix[\"close\"] = vix[\"close\"] * beta\n",
    "\n",
    "    one_min_df.drop(\n",
    "        one_min_df[one_min_df[\"date\"].dt.date.isin(unavailable_dates)].index,\n",
    "        inplace=True,\n",
    "    )\n",
    "    open_prices = (\n",
    "        one_min_df.groupby(one_min_df[\"date\"].dt.date).apply(lambda x: x.iloc[open_nth]).open.to_frame()\n",
    "    )\n",
    "    open_data = open_prices.merge(\n",
    "        vix[\"open\"].to_frame(), left_index=True, right_index=True, suffixes=(\"\", \"_vix\")\n",
    "    )\n",
    "    open_data[\"threshold_movement\"] = (open_data[\"open_vix\"] / 48) * trend_threshold\n",
    "    open_data[\"upper_bound\"] = open_data[\"open\"] * (\n",
    "        1 + open_data[\"threshold_movement\"] / 100\n",
    "    )\n",
    "    open_data[\"lower_bound\"] = open_data[\"open\"] * (\n",
    "        1 - open_data[\"threshold_movement\"] / 100\n",
    "    )\n",
    "    open_data[\"day_close\"] = one_min_df.groupby(one_min_df[\"date\"].dt.date).close.last()\n",
    "\n",
    "    daily_minute_vols = (\n",
    "        one_min_df.groupby(one_min_df[\"date\"].dt.date)\n",
    "        .apply(lambda x: x[\"close\"].pct_change().abs().mean() * 100)\n",
    "    )\n",
    "\n",
    "    daily_minute_vols_rolling = daily_minute_vols.rolling(rolling_days, min_periods=1).mean()\n",
    "\n",
    "    daily_open_to_close_trends = (\n",
    "        one_min_df.close.groupby(one_min_df[\"date\"].dt.date)\n",
    "        .apply(lambda x: (x.iloc[-1] / x.iloc[0] - 1) * 100)\n",
    "    )\n",
    "\n",
    "    daily_open_to_close_trends_rolling = daily_open_to_close_trends.abs().rolling(rolling_days, min_periods=1).mean()\n",
    "\n",
    "    rolling_ratio = daily_open_to_close_trends_rolling / daily_minute_vols_rolling\n",
    "\n",
    "    open_data.columns = [\n",
    "        \"day_open\",\n",
    "        \"open_vix\",\n",
    "        \"threshold_movement\",\n",
    "        \"upper_bound\",\n",
    "        \"lower_bound\",\n",
    "        \"day_close\",\n",
    "    ]\n",
    "    one_min_df[\n",
    "        [\n",
    "            \"day_open\",\n",
    "            \"open_vix\",\n",
    "            \"threshold_movement\",\n",
    "            \"upper_bound\",\n",
    "            \"lower_bound\",\n",
    "            \"day_close\",\n",
    "        ]\n",
    "    ] = open_data.loc[one_min_df[\"date\"].dt.date].values\n",
    "    one_min_df[\"change_from_open\"] = (\n",
    "        (one_min_df[\"close\"] / one_min_df[\"day_open\"]) - 1\n",
    "    ) * 100\n",
    "\n",
    "    def calculate_daily_trade_data(group):\n",
    "\n",
    "        \"\"\" The group is a dataframe \"\"\"\n",
    "\n",
    "        all_entries_in_a_day = {}\n",
    "        # Find the first index where the absolute price change crosses the threshold\n",
    "        entry = 1\n",
    "        while entry <= max_entries:\n",
    "            idx = group[\n",
    "                abs(group[\"change_from_open\"]) >= group[\"threshold_movement\"]\n",
    "            ].first_valid_index()\n",
    "            if idx is not None:  # if there is a crossing\n",
    "                result_dict = {\n",
    "                    \"returns\": 0,\n",
    "                    \"trigger_time\": np.nan,\n",
    "                    \"trigger_price\": np.nan,\n",
    "                    \"trend_direction\": np.nan,\n",
    "                    \"stop_loss_price\": np.nan,\n",
    "                    \"stop_loss_time\": np.nan,\n",
    "                }\n",
    "                # Record the price and time of crossing the threshold\n",
    "                cross_price = group.loc[idx, \"close\"]\n",
    "                cross_time = group.loc[idx, \"date\"]\n",
    "\n",
    "                # Determine the direction of the movement\n",
    "                direction = np.sign(group.loc[idx, \"change_from_open\"])\n",
    "\n",
    "                # Calculate the stoploss price\n",
    "\n",
    "                if stop_loss == 'dynamic':\n",
    "                    # Selecting previous days rolling ratio\n",
    "                    current_rolling_ratio = rolling_ratio.loc[:cross_time.date()].iloc[-1]\n",
    "                    # Calculating the stop_loss pct\n",
    "                    if current_rolling_ratio > 30:\n",
    "                        stop_loss_pct = 0.3\n",
    "                    elif current_rolling_ratio < 10:\n",
    "                        stop_loss_pct = 0.5\n",
    "                    else:\n",
    "                        stop_loss_pct = ((30 - current_rolling_ratio) / 100) + 0.3\n",
    "                else:\n",
    "                    stop_loss_pct = stop_loss\n",
    "                stoploss_price = cross_price * (1 - (stop_loss_pct / 100) * direction)\n",
    "                result_dict.update(\n",
    "                    {\n",
    "                        \"trigger_time\": cross_time,\n",
    "                        \"trigger_price\": cross_price,\n",
    "                        \"trend_direction\": direction,\n",
    "                        \"stop_loss_price\": stoploss_price,\n",
    "                    }\n",
    "                )\n",
    "                future_prices = group.loc[idx:, \"close\"]\n",
    "\n",
    "                if (direction == 1 and future_prices.min() <= stoploss_price) or (\n",
    "                    direction == -1 and future_prices.max() >= stoploss_price\n",
    "                ):  # Stop loss was breached\n",
    "                    result_dict[\"returns\"] = -stop_loss_pct\n",
    "                    stoploss_time_idx = (\n",
    "                        future_prices[\n",
    "                            future_prices <= stoploss_price\n",
    "                        ].first_valid_index()\n",
    "                        if direction == 1\n",
    "                        else future_prices[\n",
    "                            future_prices >= stoploss_price\n",
    "                        ].first_valid_index()\n",
    "                    )\n",
    "                    stoploss_time = group.loc[stoploss_time_idx, \"date\"]\n",
    "                    result_dict[\"stop_loss_time\"] = stoploss_time\n",
    "                    all_entries_in_a_day[f\"entry_{entry}\"] = result_dict\n",
    "                    group = group.loc[stoploss_time_idx:]\n",
    "                    entry += 1\n",
    "                else:  # Stop loss was not breached\n",
    "                    if direction == 1:\n",
    "                        result_dict[\"returns\"] = (\n",
    "                            (group[\"close\"].iloc[-1] - cross_price) / cross_price\n",
    "                        ) * 100\n",
    "                    else:\n",
    "                        result_dict[\"returns\"] = (\n",
    "                            (group[\"close\"].iloc[-1] - cross_price) / cross_price\n",
    "                        ) * -100\n",
    "                    all_entries_in_a_day[f\"entry_{entry}\"] = result_dict\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        all_entries_in_a_day[\"total_returns\"] = sum(\n",
    "            [v[\"returns\"] for v in all_entries_in_a_day.values()]\n",
    "        )\n",
    "        return all_entries_in_a_day\n",
    "\n",
    "    # Applying the function to each day's worth of data\n",
    "    returns = one_min_df.groupby(one_min_df[\"date\"].dt.date).apply(\n",
    "        calculate_daily_trade_data\n",
    "    )\n",
    "    returns = returns.to_frame()\n",
    "    returns.index = pd.to_datetime(returns.index)\n",
    "    returns.columns = [\"trade_data\"]\n",
    "\n",
    "    # merging with open_data\n",
    "    merged = returns.merge(open_data, left_index=True, right_index=True)\n",
    "    merged[\"total_returns\"] = merged[\"trade_data\"].apply(lambda x: x[\"total_returns\"])\n",
    "    merged = dm.nav_drawdown_analyser(\n",
    "        merged, column_to_convert=\"total_returns\", profit_in_pct=True\n",
    "    )\n",
    "\n",
    "    # calculating the minute vol\n",
    "    merged[\"minute_vol\"] = daily_minute_vols\n",
    "\n",
    "    # calculating the open to close trend\n",
    "    merged[\"open_to_close_trend\"] = daily_open_to_close_trends\n",
    "\n",
    "    merged[\"open_to_close_trend_abs\"] = merged[\"open_to_close_trend\"].abs()\n",
    "\n",
    "    # calculating the ratio and rolling mean\n",
    "    merged[\"minute_vol_rolling\"] = daily_minute_vols_rolling\n",
    "    merged[\"open_to_close_trend_rolling\"] = daily_open_to_close_trends_rolling\n",
    "    merged[\"ratio\"] = merged[\"open_to_close_trend_abs\"] / merged[\"minute_vol\"]\n",
    "    merged[\"rolling_ratio\"] = rolling_ratio\n",
    "\n",
    "    return merged"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
